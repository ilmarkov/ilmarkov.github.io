---
---

@article{lgreco_2022,
  title={L-GreCo: An Efficient and General Framework for Layerwise-Adaptive Gradient Compression},
  author={Alimohammadi, Mohammadreza and Markov, Ilia and Frantar, Elias and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17357},
  year={2022},
  abstract = {Data-parallel distributed training of deep neural networks (DNN) has gained very widespread adoption, but can still experience communication bottlenecks due to gradient transmission. To address this issue, entire families of lossy gradient compression mechanisms have been developed, including quantization, sparsification, and low-rank approximation, some of which are seeing significant practical adoption. Despite this progress, almost all known compression schemes apply compression uniformly across DNN layers, although layers are heterogeneous in terms of parameter count and their impact on model accuracy. In this work, we provide a general framework for adapting the degree of compression across the model's layers dynamically during training, significantly improving the overall compression without sacrificing accuracy. Our framework, called L-GreCo, is based on an efficient adaptive algorithm, which automatically picks the optimal compression parameters for model layers guaranteeing the best compression ratio while respecting a theoretically-justified error constraint. Our extensive experimental study over image classification and language modeling tasks shows that L-GreCo is effective across all three compression families, and achieves up to 2.5× training speedup and up to 5× compression improvement over efficient implementations of standard approaches while recovering full accuracy. Moreover, we show that L-GreCo is complementary to existing adaptive algorithms improving their compression ratio by 50% and practical throughput by 66%.},
  html= {https://arxiv.org/abs/2210.17357},
  note = {Submitted to MlSys'23}
}

@inproceedings{10.1145/3528535.3565248,
author = {Markov, Ilia and Ramezanikebrya, Hamidreza and Alistarh, Dan},
title = {CGX: Adaptive System Support for Communication-Efficient Deep Learning},
year = {2022},
isbn = {9781450393409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528535.3565248},
doi = {10.1145/3528535.3565248},
abstract = {The ability to scale out training workloads has been one of the key performance enablers of deep learning. The main scaling approach is data-parallel GPU-based training, which has been boosted by hardware and software support for highly efficient point-to-point communication, and in particular via hardware bandwidth over-provisioning. Overprovisioning comes at a cost: there is an order of magnitude price difference between "cloud-grade" servers with such support, relative to their popular "consumer-grade" counterparts, although single server-grade and consumer-grade GPUs can have similar computational envelopes.In this paper, we show that the costly hardware overprovisioning approach can be supplanted via algorithmic and system design, and propose a framework called CGX, which provides efficient software support for compressed communication in ML applications, for both multi-GPU single-node training, as well as larger-scale multi-node training. CGX is based on two technical advances: At the system level, it relies on a re-developed communication stack for ML frameworks, which provides flexible, highly-efficient support for compressed communication. At the application level, it provides seamless, parameter-free integration with popular frameworks, so that end-users do not have to modify training recipes, nor significant training code. This is complemented by a layer-wise adaptive compression technique which dynamically balances compression gains with accuracy preservation. CGX integrates with popular ML frameworks, providing up to 3X speedups for multi-GPU nodes based on commodity hardware, and order-of-magnitude improvements in the multi-node setting, with negligible impact on accuracy.},
booktitle = {Proceedings of the 23rd Conference on 23rd ACM/IFIP International Middleware Conference},
pages = {241–254},
numpages = {14},
keywords = {gradients compression, deep learning, distributed systems},
location = {Quebec, QC, Canada},
series = {Middleware '22},
html = {https://dl.acm.org/doi/abs/10.1145/3528535.3565248},
note = {Best Paper Award Runner-Up},
bibtex_show = {true},
}

@article{Nadiradze_Markov_Chatterjee_Kungurtsev_Alistarh_2021,
 title={Elastic Consistency: A Practical Consistency Model for Distributed Stochastic Gradient Descent},
 volume={35},
 url={https://ojs.aaai.org/index.php/AAAI/article/view/17092},
 DOI={10.1609/aaai.v35i10.17092},
 abstract = {One key element behind the recent progress of machine learning has been the ability to train machine learning models in large-scale distributed shared-memory and message-passing environments. Most of these models are trained employing variants of stochastic gradient descent (SGD) based optimization, but most methods involve some type of consistency relaxation relative to sequential SGD, to mitigate its large communication or synchronization costs at scale. In this paper, we introduce a general consistency condition covering communication-reduced and asynchronous distributed SGD implementations. Our framework, called elastic consistency, decouples the system-specific aspects of the implementation from the SGD convergence requirements, giving a general way to obtain convergence bounds for a wide variety of distributed SGD methods used in practice. Elastic consistency can be used to re-derive or improve several previous convergence bounds in message-passing and shared-memory settings, but also to analyze new models and distribution schemes. As a direct application, we propose and analyze a new synchronization-avoiding scheduling scheme for distributed SGD, and show that it can be used to efficiently train deep convolutional models for image classification.},
 number={10},
 journal={Proceedings of the AAAI Conference on Artificial Intelligence},
 author={Nadiradze, Giorgi and Markov, Ilia and Chatterjee, Bapi and Kungurtsev, Vyacheslav and Alistarh, Dan},
 year={2021},
 month={May},
 pages={9037-9045},
 html = {https://ojs.aaai.org/index.php/AAAI/article/view/17092},
 bibtex_show = {true},
}

@article{ramizani_kebrya_nuqsgd_2021,
author = {Ramezani-Kebrya, Ali and Faghri, Fartash and Markov, Ilia and Aksenov, Vitalii and Alistarh, Dan and Roy, Daniel M.},
title = {NUQSGD: Provably Communication-Efficient Data-Parallel SGD via Nonuniform Quantization},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {As the size and complexity of models and datasets grow, so does the need for communication efficient variants of stochastic gradient descent that can be deployed to perform parallel model training. One popular communication-compression method for data-parallel SGD is QSGD (Alistarh et al., 2017), which quantizes and encodes gradients to reduce communication costs. The baseline variant of QSGD provides strong theoretical guarantees, however, for practical purposes, the authors proposed a heuristic variant which we call QSGDinf, which demonstrated impressive empirical gains for distributed training of large neural networks. In this paper, we build on this work to propose a new gradient quantization scheme, and show that it has both stronger theoretical guarantees than QSGD, and matches and exceeds the empirical performance of the QSGDinf heuristic and of other compression methods.},
journal = {J. Mach. Learn. Res.},
month = {jul},
articleno = {114},
numpages = {43},
bibtex_show = {true},
keywords = {gradient compression, quantization, communication-efficient SGD, deep learning, dataparallel SGD},
html = {https://dl.acm.org/doi/abs/10.5555/3546258.3546372}
}

@inproceedings{faghri_adaptive_2020,
 author = {Faghri, Fartash and Tabrizian, Iman and Markov, Ilia and Alistarh, Dan and Roy, Daniel M and Ramezani-Kebrya, Ali},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {3174--3185},
 publisher = {Curran Associates, Inc.},
 title = {Adaptive Gradient Quantization for Data-Parallel SGD},
 url = {https://proceedings.neurips.cc/paper/2020/file/20b5e1cf8694af7a3c1ba4a87f073021-Paper.pdf},
 volume = {33},
 year = {2020},
 abstract = {Many communication-efficient variants of SGD use gradient quantization schemes. These schemes are often heuristic and fixed over the course of training. We empirically observe that the statistics of gradients of deep models change during the training. Motivated by this observation, we introduce two adaptive quantization schemes, ALQ and AMQ. In both schemes, processors update their compression schemes in parallel by efficiently computing sufficient statistics of a parametric distribution. We improve the validation accuracy by almost 2% on CIFAR-10 and 1% on ImageNet in challenging low-cost communication setups. Our adaptive methods are also significantly more robust to the choice of hyperparameters.},
 bibtex_show = {true},
 html = {https://proceedings.neurips.cc/paper/2020/hash/20b5e1cf8694af7a3c1ba4a87f073021-Abstract.html}
}


@INPROCEEDINGS{klein_2021,
  author={Klein, Karen and Pascual-Perez, Guillermo and Walter, Michael and Kamath, Chethan and Capretto, Margarita and Cueto, Miguel and Markov, Ilia and Yeo, Michelle and Alwen, Joël and Pietrzak, Krzysztof},
  booktitle={2021 IEEE Symposium on Security and Privacy (SP)},
  title={Keep the Dirt: Tainted TreeKEM, Adaptively and Actively Secure Continuous Group Key Agreement},
  year={2021},
  volume={},
  number={},
  abstract = {While messaging systems with strong security guarantees are widely used in practice, designing a protocol that scales efficiently to large groups and enjoys similar security guarantees remains largely open. The two existing proposals to date are ART (Cohn-Gordon et al., CCS18) and TreeKEM (IETF, The Messaging Layer Security Protocol, draft). TreeKEM is the currently considered candidate by the IETF MLS working group, but dynamic group operations (i.e. adding and removing users) can cause efficiency issues. In this paper we formalize and analyze a variant of TreeKEM which we term Tainted TreeKEM (TTKEM for short). The basic idea underlying TTKEM was suggested by Millican (MLS mailing list, February 2018). This version is more efficient than TreeKEM for some natural distributions of group operations, we quantify this through simulations.Our second contribution is two security proofs for TTKEM which establish post compromise and forward secrecy even against adaptive attackers. The security loss (to the underlying PKE) in the Random Oracle Model is a polynomial factor, and a quasipolynomial one in the Standard Model. Our proofs can be adapted to TreeKEM as well. Before our work no security proof for any TreeKEM-like protocol establishing tight security against an adversary who can adaptively choose the sequence of operations was known. We also are the first to prove (or even formalize) active security where the server can arbitrarily deviate from the protocol specification. Proving fully active security – where also the users can arbitrarily deviate – remains open.},
  pages={268-284},
  doi={10.1109/SP40001.2021.00035},
  bibtex_show = {true},
  html = {https://ieeexplore.ieee.org/document/9519462},
  note = {Work is done during the rotation in Pietrzak group}
}